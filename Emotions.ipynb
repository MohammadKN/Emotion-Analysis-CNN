{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba369ae0-f7bc-48d2-b391-1c7e597d9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:/Users/User/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:/Users/User/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:/Users/User/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:/Users/User/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:/Users/User/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('punkt', download_dir='C:/Users/User/nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='C:/Users/User/nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:/Users/User/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='C:/Users/User/nltk_data')\n",
    "nltk.download('averaged_perceptron_tagger_eng', download_dir='C:/Users/User/nltk_data') \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bedaad0d-bc51-445b-9943-7b3bc822a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "index                                                          \n",
      "0          i just feel really helpless and heavy hearted      4\n",
      "1      ive enjoyed being able to slouch about relax a...      0\n",
      "2      i gave up my internship with the dmrg and am f...      4\n",
      "3                             i dont know i feel so lost      0\n",
      "4      i am a kindergarten teacher and i am thoroughl...      4\n",
      "...                                                  ...    ...\n",
      "4995       i long every moment to feel your gentle touch      2\n",
      "4996   im just not feeling that needy or messed up an...      0\n",
      "4997   i hate him and the feeling is pretty mutual i ...      3\n",
      "4998   i can guarantee you that if i saw these jerks ...      4\n",
      "4999   i know for a normal girl it makes her feel off...      3\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"text.csv\")\n",
    "df = df.head(5000)\n",
    "if df.columns[0] == \"Unnamed: 0\":\n",
    "    df.rename(columns={\"Unnamed: 0\": \"index\"}, inplace=True)\n",
    "df.set_index(\"index\", inplace= True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ca73c5-fa34-4917-a6bd-1c95e5536e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence = sentence.split()\n",
    "    tokens = word_tokenize(\" \".join (sentence))\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]\n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73fda1d3-245a-44e4-986c-622afb9e7955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = {w.strip().lower() for w in open(\"stopwords.txt\")}\n",
    "tokens = []\n",
    "\n",
    "def clean_text(text, stop_set=stopwords):\n",
    "    row_tokens = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "    row_tokens = [w for w in row_tokens if w not in stop_set]\n",
    "    tokens.extend(row_tokens)\n",
    "    return \" \".join(row_tokens)\n",
    "\n",
    "def remove_ing(word):\n",
    "    return re.sub(r'ing$', '', word)\n",
    "\n",
    "df[\"lemmatized_text\"] = df[\"text\"].apply(lemmatize_sentence)\n",
    "df[\"lemmatized_text\"] = df[\"lemmatized_text\"].apply(lambda sentence: \" \".join(remove_ing(word) for word in sentence.split()))\n",
    "df[\"lemmatized_text\"] = df[\"lemmatized_text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d287dc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label  \\\n",
      "index                                                             \n",
      "0          i just feel really helpless and heavy hearted      4   \n",
      "1      ive enjoyed being able to slouch about relax a...      0   \n",
      "2      i gave up my internship with the dmrg and am f...      4   \n",
      "3                             i dont know i feel so lost      0   \n",
      "4      i am a kindergarten teacher and i am thoroughl...      4   \n",
      "...                                                  ...    ...   \n",
      "4995       i long every moment to feel your gentle touch      2   \n",
      "4996   im just not feeling that needy or messed up an...      0   \n",
      "4997   i hate him and the feeling is pretty mutual i ...      3   \n",
      "4998   i can guarantee you that if i saw these jerks ...      4   \n",
      "4999   i know for a normal girl it makes her feel off...      3   \n",
      "\n",
      "                                         lemmatized_text  \n",
      "index                                                     \n",
      "0                       just feel helpless heavy hearted  \n",
      "1      enjoy able slouch relax unwind frankly need la...  \n",
      "2                   give internship dmrg feel distraught  \n",
      "3                                    dont know feel lose  \n",
      "4      kindergarten teacher weary job take university...  \n",
      "...                                                  ...  \n",
      "4995                       long moment feel gentle touch  \n",
      "4996                        just feel needy mess anymore  \n",
      "4997   hate feel pretty mutual find obnoxious think b...  \n",
      "4998           can guarantee saw jerk public feel afraid  \n",
      "4999   know normal girl feel offended shed just ignor...  \n",
      "\n",
      "[5000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a60191",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = pd.Series(tokens).value_counts()\n",
    "vc = vc.head(7000)\n",
    "words = list(vc.index.unique())\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd9df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index = 1 \n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = index\n",
    "        index += 1\n",
    "print(\"Vocabulary:\", word_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31267a-435a-4d81-9776-63f5546e4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = text.split()\n",
    "emotion_related_words = []\n",
    "new_word = \"feel\"\n",
    "emotion_related_words.extend(get_synonyms(word))\n",
    "emotion_related_words.extend(get_antonyms(word))\n",
    "emotion_related_words.extend(get_hyponyms(word))\n",
    "emotion_related_words.extend(get_hypernyms(word))\n",
    "\n",
    "print(set(get_synonyms(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e61624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣  Build / extend the vocabulary\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "word_to_index = {}             # or load your existing dict here\n",
    "for tok in special_tokens:      # make sure the specials are present first\n",
    "    if tok not in word_to_index:\n",
    "        word_to_index[tok] = len(word_to_index)\n",
    "\n",
    "# Optionally add your corpus words afterwards\n",
    "for sent in sentences:          # sentences is a list of token lists\n",
    "    for word in sent:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "\n",
    "# 2️⃣  Keep the index of <UNK> handy\n",
    "UNK_IDX = word_to_index[\"<UNK>\"]\n",
    "\n",
    "# 3️⃣  Robust mapping function\n",
    "def sentence_to_sequence(sentence, word_to_index, unk_index=UNK_IDX):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their indices.\n",
    "    Unknown words map to `unk_index`.\n",
    "    \"\"\"\n",
    "    return [word_to_index.get(word, unk_index) for word in sentence]\n",
    "\n",
    "# 4️⃣  Vectorise every sentence\n",
    "sequences = [sentence_to_sequence(sent, word_to_index) for sent in sentences]\n",
    "print(\"Sequences:\", sequences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717621c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for sentence in df['text']:\n",
    "    all_words.extend(clean_text(sentence))\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "word2idx = {word: idx+2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "def text_to_sequence(text):\n",
    "    return [word2idx.get(word, word2idx['<UNK>']) for word in clean_text(text)]\n",
    "\n",
    "\n",
    "df['sequence'] = df['text'].apply(text_to_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b040fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    padded = np.zeros((len(sequences), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded[i, :length] = seq[:length]\n",
    "    return padded\n",
    "\n",
    "# Example: set max_len (you can use 50, 100, or based on your data)\n",
    "max_len = 50  # try different values if needed\n",
    "\n",
    "# Apply padding\n",
    "padded_array = pad_sequences(df['sequence'], max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffaa9a-aa69-4548-8b4d-ac12a618ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sadness = []\n",
    "list_joy = []\n",
    "list_love = []\n",
    "list_anger = []\n",
    "list_fear = []\n",
    "list_surprise = []\n",
    "list_unlabeled = []\n",
    "def categorize_emotions(row):\n",
    "    if row['label'] == 0:\n",
    "        list_sadness.append(row['text'])\n",
    "    elif row['label'] == 1:\n",
    "        list_joy.append(row['text'])\n",
    "    elif row['label'] == 2:\n",
    "        list_love.append(row['text'])\n",
    "    elif row['label'] == 3:\n",
    "        list_anger.append(row['text'])\n",
    "    elif row['label'] == 4:\n",
    "        list_fear.append(row['text'])\n",
    "    elif row['label'] == 5:\n",
    "        list_surprise.append(row['text'])\n",
    "    return row\n",
    "df = df.apply(categorize_emotions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc203c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sadness = pd.DataFrame({'text': list_sadness, 'label': 0})\n",
    "joy = pd.DataFrame({'text': list_joy, 'label': 1})\n",
    "love = pd.DataFrame({'text': list_love, 'label': 2})\n",
    "anger = pd.DataFrame({'text': list_anger, 'label': 3})\n",
    "fear = pd.DataFrame({'text': list_fear, 'label': 4})\n",
    "surprise = pd.DataFrame({'text': list_surprise, 'label': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = pd.concat([sadness, joy, love, anger, fear, surprise], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sadness_tokens  = []\n",
    "joy_tokens      = []\n",
    "love_tokens     = []\n",
    "anger_tokens    = []\n",
    "fear_tokens     = []\n",
    "surprise_tokens = []\n",
    "\n",
    "def tokenize_sadness(row):\n",
    "    token = row['text'].split()\n",
    "    sadness_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_joy(row):\n",
    "    token = row['text'].split()\n",
    "    joy_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_love(row):\n",
    "    token = row['text'].split()\n",
    "    love_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_anger(row):\n",
    "    token = row['text'].split()\n",
    "    anger_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_fear(row):\n",
    "    token = row['text'].split()\n",
    "    fear_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_surprise(row):\n",
    "    token = row['text'].split()\n",
    "    surprise_tokens.extend(token)\n",
    "    return row\n",
    "sadness = sadness.apply(tokenize_sadness, axis=1)\n",
    "joy = joy.apply(tokenize_joy, axis=1)\n",
    "love = love.apply(tokenize_love, axis=1)\n",
    "anger = anger.apply(tokenize_anger, axis=1)\n",
    "fear = fear.apply(tokenize_fear, axis=1)\n",
    "surprise = surprise.apply(tokenize_surprise, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ee467",
   "metadata": {},
   "outputs": [],
   "source": [
    "sadness_words_vc  = pd.Series(sadness_tokens).value_counts()\n",
    "joy_words_vc      = pd.Series(joy_tokens).value_counts()\n",
    "love_words_vc     = pd.Series(love_tokens).value_counts()\n",
    "anger_words_vc    = pd.Series(anger_tokens).value_counts()\n",
    "fear_words_vc     = pd.Series(fear_tokens).value_counts()\n",
    "surprise_words_vc = pd.Series(surprise_tokens).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af75214",
   "metadata": {},
   "outputs": [],
   "source": [
    "sadness_words_vc.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_words_vc.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "love_words_vc.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c474b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anger_words_vc.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_words_vc.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddade2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_words_vc.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5d76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6bb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sadness:\",  len(sadness) / 416809 * 100)\n",
    "print(\" \")\n",
    "print(\"joy:\", len(joy) / 416809 * 100)\n",
    "print(\" \")\n",
    "print(\"love:\",  len(love) / 416809 * 100)\n",
    "print(\" \")\n",
    "print(\"anger:\",  len(anger) / 416809 * 100)\n",
    "print(\" \")\n",
    "print(\"fear:\",len(fear) / 416809 * 100)\n",
    "print(\" \")\n",
    "print(\"surprise:\",  len(surprise) / 416809 * 100)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549334ca-45e8-4d05-9d2f-1de430d36f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = \"\"\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return list(synonyms)\n",
    "\n",
    "def get_antonyms(word):\n",
    "    antonyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            for ant in lemma.antonyms():\n",
    "                antonyms.add(ant.name().lower())\n",
    "    return list(antonyms)\n",
    "\n",
    "def get_hypernyms(word):\n",
    "    hypernyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for hyper in syn.hypernyms():\n",
    "            for lemma in hyper.lemmas():\n",
    "                hypernyms.add(lemma.name().lower())\n",
    "    return list(hypernyms)\n",
    "\n",
    "def get_hyponyms(word):\n",
    "    hyponyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for hypo in syn.hyponyms():\n",
    "            for lemma in hypo.lemmas():\n",
    "                hyponyms.add(lemma.name().lower())\n",
    "    return list(hyponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c23cb-3080-4a43-ac83-94f7066865ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
