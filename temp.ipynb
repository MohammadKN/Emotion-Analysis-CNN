{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f463e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "I have python cells that are all merged together I want you to split them properly\n",
    "\n",
    "1. Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "2. Loading the dataset\n",
    "df = pd.read_csv(\"text.csv\")\n",
    "df.head()\n",
    "3. Preprocessing the data\n",
    "3.1 checking for missing values\n",
    "df.isnull().sum()\n",
    "3.2 checking for duplicates\n",
    "df.duplicated().sum()\n",
    "3.3 displaying dataframe shape\n",
    "print(\"Shape of the data: \", df.shape)\n",
    "df.head()\n",
    "3.4 removing unnecessary columns\n",
    "if df.columns[0] == \"Unnamed: 0\":\n",
    "    df.rename(columns={\"Unnamed: 0\": \"index\"}, inplace=True)\n",
    "    df.set_index(\"index\", inplace= True)\n",
    "df.head()\n",
    "3.5 displaying class distribution\n",
    "After we found that the dataset is imbalanced, we will use synonym replacement to balance the dataset.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = []\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "3.6 Synonym replacement\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return list(synonyms)\n",
    "\n",
    "3.7 lower case letters, removing special characters, and removing extra spaces, and then lemmatizing the text.\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [tok for tok in tokens if tok.isalpha()]\n",
    "    tokens = [tok for tok in tokens if tok not in sw]\n",
    "    tokens = word_tokenize(\" \".join (tokens))\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "print(df)\n",
    "3.8 Identifying the POS of a word.\n",
    "df['label'] = df['label'].replace(0,'Sadness')\n",
    "df['label'] = df['label'].replace(1,'Joy')\n",
    "df['label'] = df['label'].replace(2,'Love')\n",
    "df['label'] = df['label'].replace(3,'Anger')\n",
    "df['label'] = df['label'].replace(4,'Fear')\n",
    "df['label'] = df['label'].replace(5,'Surprise')\n",
    "\n",
    "count = df['label'].value_counts()\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,s 6), facecolor='white')\n",
    "\n",
    "# Plot pie chart on the first subplot\n",
    "palette = sns.color_palette(\"viridis\")\n",
    "sns.set_palette(palette)\n",
    "axs[1].pie(count, labels=count.index, autopct='%1.1f%%', startangle=140)\n",
    "axs[1].set_title('Distribution of Categories')\n",
    "\n",
    "# Plot bar chart on the second subplot\n",
    "sns.barplot(x=count.index, y=count.values, ax=axs[0], palette=\"viridis\")\n",
    "axs[0].set_title('Count of Categories')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "3.9: Augmenting the Minority Class to Balance the Dataset\n",
    "df.head()\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name().lower() != word.lower():\n",
    "                synonyms.add(lemma.name().replace('_', ' '))\n",
    "    return list(synonyms)\n",
    "\n",
    "get_synonyms('sad')\n",
    "\n",
    "def augment_sentence(text, num_augments=2):\n",
    "    words = text.split()\n",
    "    new_sentences = [text]\n",
    "    for _ in range(num_augments):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            # If it's an emotion-related word, try replacing it\n",
    "            if word.lower() in text:\n",
    "                syns = get_synonyms(word)\n",
    "                if syns:\n",
    "                    new_word = random.choice(syns)\n",
    "                    new_words.append(new_word)\n",
    "                else:\n",
    "                    new_words.append(word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        new_sentences.append(' '.join(new_words))\n",
    "    return new_sentences\n",
    "new_rows = []\n",
    "\n",
    "df_surprise = df[df['label'] == 'Surprise'].copy()\n",
    "\n",
    "for id, row in df_surprise.iterrows():\n",
    "    augmentations = augment_sentence(row['text'])\n",
    "    for sent in augmentations:\n",
    "        new_rows.append({\n",
    "                'text':  sent,\n",
    "                'label': 'Surprise'\n",
    "            })\n",
    "\n",
    "print(df.value_counts())\n",
    "df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "print(df.value_counts())\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    augments = augment_sentence(row['text'], num_augments=2)\n",
    "    augmented_texts.extend(augments)\n",
    "    augmented_labels.extend([row['label']] * len(augments))\n",
    "df.head()\n",
    "vc = pd.Series(tokens).value_counts()\n",
    "vc = vc.head(7000)\n",
    "words = list(vc.index.unique())\n",
    "print(len(words))\n",
    "word_to_index = {}\n",
    "index = 1 \n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = index\n",
    "        index += 1\n",
    "print(\"Vocabulary:\", word_to_index)\n",
    "\n",
    "# 1️⃣  Build / extend the vocabulary\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "word_to_index = {}             # or load your existing dict here\n",
    "for tok in special_tokens:      # make sure the specials are present first\n",
    "    if tok not in word_to_index:\n",
    "        word_to_index[tok] = len(word_to_index)\n",
    "\n",
    "# Optionally add your corpus words afterwards\n",
    "for sent in sentences:          # sentences is a list of token lists\n",
    "    for word in sent:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "\n",
    "# 2️⃣  Keep the index of <UNK> handy\n",
    "UNK_IDX = word_to_index[\"<UNK>\"]\n",
    "\n",
    "# 3️⃣  Robust mapping function\n",
    "def sentence_to_sequence(sentence, word_to_index, unk_index=UNK_IDX):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their indices.\n",
    "    Unknown words map to `unk_index`.\n",
    "    \"\"\"\n",
    "    return [word_to_index.get(word, unk_index) for word in sentence]\n",
    "\n",
    "# 4️⃣  Vectorise every sentence\n",
    "sequences = [sentence_to_sequence(sent, word_to_index) for sent in sentences]\n",
    "print(\"Sequences:\", sequences[:10])\n",
    "\n",
    "all_words = []\n",
    "for sentence in df['text']:\n",
    "    all_words.extend(clean_text(sentence))\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "word2idx = {word: idx+2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "def text_to_sequence(text):\n",
    "    return [word2idx.get(word, word2idx['<UNK>']) for word in clean_text(text)]\n",
    "\n",
    "\n",
    "df['sequence'] = df['text'].apply(text_to_sequence)\n",
    "df['sequence']\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    padded = np.zeros((len(sequences), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded[i, :length] = seq[:length]\n",
    "    return padded\n",
    "\n",
    "# Example: set max_len (you can use 50, 100, or based on your data)\n",
    "max_len = 50  # try different values if needed\n",
    "\n",
    "# Apply padding\n",
    "padded_array = pad_sequences(df['sequence'], max_len)\n",
    "padded_array\n",
    "list_sadness = []\n",
    "list_joy = []\n",
    "list_love = []\n",
    "list_anger = []\n",
    "list_fear = []\n",
    "list_surprise = []\n",
    "list_unlabeled = []\n",
    "def categorize_emotions(row):\n",
    "    if row['label'] == 0:\n",
    "        list_sadness.append(row['text'])\n",
    "    elif row['label'] == 1:\n",
    "        list_joy.append(row['text'])\n",
    "    elif row['label'] == 2:\n",
    "        list_love.append(row['text'])\n",
    "    elif row['label'] == 3:\n",
    "        list_anger.append(row['text'])\n",
    "    elif row['label'] == 4:\n",
    "        list_fear.append(row['text'])\n",
    "    elif row['label'] == 5:\n",
    "        list_surprise.append(row['text'])\n",
    "    return row\n",
    "df = df.apply(categorize_emotions, axis=1)\n",
    "sadness = pd.DataFrame({'text': list_sadness, 'label': 0})\n",
    "joy = pd.DataFrame({'text': list_joy, 'label': 1})\n",
    "love = pd.DataFrame({'text': list_love, 'label': 2})\n",
    "anger = pd.DataFrame({'text': list_anger, 'label': 3})\n",
    "fear = pd.DataFrame({'text': list_fear, 'label': 4})\n",
    "surprise = pd.DataFrame({'text': list_surprise, 'label': 5})\n",
    "sorted_df = pd.concat([sadness, joy, love, anger, fear, surprise], ignore_index=True)\n",
    "\n",
    "print(sorted_df)\n",
    "sadness_tokens  = []\n",
    "joy_tokens      = []\n",
    "love_tokens     = []\n",
    "anger_tokens    = []\n",
    "fear_tokens     = []\n",
    "surprise_tokens = []\n",
    "\n",
    "def tokenize_sadness(row):\n",
    "    token = row['text'].split()\n",
    "    sadness_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_joy(row):\n",
    "    token = row['text'].split()\n",
    "    joy_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_love(row):\n",
    "    token = row['text'].split()\n",
    "    love_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_anger(row):\n",
    "    token = row['text'].split()\n",
    "    anger_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_fear(row):\n",
    "    token = row['text'].split()\n",
    "    fear_tokens.extend(token)\n",
    "    return row\n",
    "def tokenize_surprise(row):\n",
    "    token = row['text'].split()\n",
    "    surprise_tokens.extend(token)\n",
    "    return row\n",
    "sadness = sadness.apply(tokenize_sadness, axis=1)\n",
    "joy = joy.apply(tokenize_joy, axis=1)\n",
    "love = love.apply(tokenize_love, axis=1)\n",
    "anger = anger.apply(tokenize_anger, axis=1)\n",
    "fear = fear.apply(tokenize_fear, axis=1)\n",
    "surprise = surprise.apply(tokenize_surprise, axis=1)\n",
    "\n",
    "sadness_words_vc  = pd.Series(sadness_tokens).value_counts()\n",
    "joy_words_vc      = pd.Series(joy_tokens).value_counts()\n",
    "love_words_vc     = pd.Series(love_tokens).value_counts()\n",
    "anger_words_vc    = pd.Series(anger_tokens).value_counts()\n",
    "fear_words_vc     = pd.Series(fear_tokens).value_counts()\n",
    "surprise_words_vc = pd.Series(surprise_tokens).value_counts()\n",
    "#1 Input, 1 output Neural Network\n",
    "\n",
    "\n",
    "def sigmoid (SOP):\n",
    "    return 1 / ( 1 + np.exp(-1 * SOP))\n",
    "\n",
    "def error (predicted, target):\n",
    "    return np.power((predicted - target) , 2)\n",
    "def error_predicted_derivative (predicted, target):\n",
    "    return 2 * (predicted - target)\n",
    "def activation_sop_derivative (SOP):\n",
    "    return sigmoid(SOP) * (1 - sigmoid(SOP))\n",
    "def sop_w_deriv (x):\n",
    "    return x\n",
    "def update_weight (weight, learning_rate, grad):\n",
    "    return weight - (learning_rate * grad)\n",
    "\n",
    "input_layer = np.random.rand(10)\n",
    "input_size = len(input_layer)\n",
    "weights = np.random.rand(10)\n",
    "target = 0.3\n",
    "learning_rate = 0.0005\n",
    "x = float\n",
    "b = np.random.rand(1)\n",
    "print(input_layer)\n",
    "print(\"target: \", target)\n",
    "predictions = []\n",
    "errors = []\n",
    "printed = 0\n",
    "\n",
    "for i in range(80000):\n",
    "    sop = np.sum(weights * input_layer)\n",
    "    predicted = sigmoid(sop)\n",
    "    err = error(predicted, target)\n",
    "    for j in range(input_size):\n",
    "        grad = error_predicted_derivative(predicted, target) * activation_sop_derivative(sop) * sop_w_deriv(input_layer[j])\n",
    "        weights[j] = update_weight(weights[j], learning_rate, grad)\n",
    "    predictions.append(predicted)\n",
    "    errors.append(err)\n",
    "    if err < 0.0001 and not printed:\n",
    "        print(\"Got the right answer after {} iterations.\".format(i))\n",
    "        printed = 1\n",
    "        \n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(predictions)\n",
    "plt.title(\"Predictions over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(errors)\n",
    "plt.title(\"Error Rate over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
